<h1 align="center">
  <br>
  <!-- <a href="http://www.amitmerchant.com/electron-markdownify"><img src="https://raw.githubusercontent.com/amitmerchant1990/electron-markdownify/master/app/img/markdownify.png" alt="Markdownify" width="200"></a>
  <br> -->
  Llama Resume Chatbot (üõ†Ô∏è Under Construction)
  <br>
</h1>

<h4 align="center">A RAG chatbot to chat with Resume and extension to connect on LinkedIn</h4>

<!-- <p align="center">
  <a href="https://badge.fury.io/js/electron-markdownify">
    <img src="https://badge.fury.io/js/electron-markdownify.svg"
         alt="Gitter">
  </a>
  <a href="https://gitter.im/amitmerchant1990/electron-markdownify"><img src="https://badges.gitter.im/amitmerchant1990/electron-markdownify.svg"></a>
  <a href="https://saythanks.io/to/bullredeyes@gmail.com">
      <img src="https://img.shields.io/badge/SayThanks.io-%E2%98%BC-1EAEDB.svg">
  </a>
  <a href="https://www.paypal.me/AmitMerchant">
    <img src="https://img.shields.io/badge/$-donate-ff69b4.svg?maxAge=2592000&amp;style=flat">
  </a>
</p> -->

<p align="center">
  <a href="#overview">Overview</a> ‚Ä¢
  <a href="#architecture">Architecture</a> ‚Ä¢
  <a href="#chatbot">Chatbot</a> ‚Ä¢
  <a href="#chrome-extension">Chrome Extension</a> ‚Ä¢
  <a href="#serve-ollama">Serve Ollama</a> ‚Ä¢
  <a href="#build-from-source">Build from Source</a> ‚Ä¢
  <a href="#license">Contact</a>
</p>

## üìã Overview

## ‚öôÔ∏è Architecture

## üí¨ Chatbot

## üåê Chrome Extension

## Serve Ollama
1. Download and install **Ollama** from https://ollama.com/download

2. Pull required open-source LLMs (here we use [`llama3`](https://ollama.com/library/llama3), you can use other models like [`mistral`](https://ollama.com/library/mistral), [`llama2-uncensored`](https://ollama.com/library/llama2-uncensored), etc.)
```bash
ollama pull llama3
```

3. Pull required embedding models (here we use [`nomic-embed-text`](https://ollama.com/library/nomic-embed-text))
```bash
ollama pull nomic-embed-text
```

4. Serve Ollama locally (by default Ollama is served from `http://localhost:11434`)
```bash
ollama serve
```

Note: If this command results in an error, make sure to quit any running Ollama background processes.

## Build from Source
1. Clone the repository
```bash
git clone https://github.com/zzarif/Llama-Resume-Chatbot.git
cd Llama-Resume-Chatbot/
```

2. Install necessary dependencies
```bash
poetry install
```

3. Activate virtual environment
```bash
poetry shell
```

4. Start chatbot backend server (served from `http://localhost:8000`)
```bash
python chatbot/backend/api.py
```

5. Launch the chatbot (served from `http://localhost:8501`)
```bash
streamlit run chatbot/main.py
```

## üåê Contact:
[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?logo=linkedin&logoColor=white)](https://www.linkedin.com/in/zibran-zarif-amio-b82717263/) [![Mail](https://img.shields.io/badge/Gmail-EA4335?logo=gmail&logoColor=fff)](mailto:zibran.zarif.amio@gmail.com)